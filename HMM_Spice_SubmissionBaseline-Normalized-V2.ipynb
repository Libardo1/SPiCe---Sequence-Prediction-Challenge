{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from math import expm1\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeInitialProb(trainDataFile,numOfStates):\n",
    "    trainFile=open(trainDataFile,\"r\")\n",
    "    metaDataLine = trainFile.readline()\n",
    "    headerLine = metaDataLine.split(\" \")\n",
    "    numSequences = int(headerLine[0])\n",
    "    distinctObservations= int(headerLine[1])#Total Number of Distinct Observations\n",
    "    numOfStates=min(numOfStates,distinctObservations)\n",
    "    empiricalCount=np.zeros(shape=numOfStates)\n",
    "    empiricalFreq=defaultdict(int)\n",
    "    for n in range(numSequences):\n",
    "        line = trainFile.readline()#Reading Sequences 1 by 1\n",
    "        l = line.split(\" \")\n",
    "        startState=int(l[1])\n",
    "        empiricalFreq[startState] = empiricalFreq[startState]+1\n",
    "    totalObservations=0\n",
    "    for i in np.arange(numOfStates):\n",
    "        empiricalCount[i]=empiricalFreq[i]\n",
    "        totalObservations=totalObservations+empiricalCount[i]\n",
    "    initialProb=[count/totalObservations for count in empiricalCount]\n",
    "    return (numOfStates,distinctObservations,initialProb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createRandomMatrixA(numOfStates):\n",
    "    matrixA=np.zeros(shape=(numOfStates,numOfStates),dtype=float)\n",
    "    prob=1.0/(numOfStates*numOfStates)\n",
    "    for i in np.arange(numOfStates):\n",
    "        for j in np.arange(numOfStates):\n",
    "            matrixA[i][j]=prob\n",
    "    return matrixA\n",
    "def createRandomMatrixB(numOfStates,distinctObservations):\n",
    "    matrixB=np.zeros(shape=(numOfStates,distinctObservations),dtype=float)\n",
    "    prob=1.0/(numOfStates*distinctObservations)\n",
    "    for i in np.arange(numOfStates):\n",
    "        for j in np.arange(distinctObservations):\n",
    "            matrixB[i][j]=prob\n",
    "    return matrixB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def computeAlpha(observations,a,b,pi,alphaDP):\n",
    "    statesC=a.shape[0]\n",
    "    timePts=observations.shape[0]\n",
    "    if timePts<1:\n",
    "        return\n",
    "    alphaDpScaleTime0=0    \n",
    "    for i in np.arange(statesC):\n",
    "        alphaDP[0][i]=pi[i]*b[i][observations[0]]\n",
    "        alphaDpScaleTime0+=alphaDP[0][i]\n",
    "    for i in np.arange(statesC):\n",
    "        alphaDP[0][i]/=alphaDpScaleTime0\n",
    "    #print(\"Initial Alpha \",alphaDP[0])\n",
    "    for t in np.arange(1,timePts):\n",
    "        alphaDpScaleTimeT=0\n",
    "        for i in np.arange(statesC):\n",
    "            for j in np.arange(statesC):\n",
    "                alphaDP[t][i]+=alphaDP[t-1][j]*a[j][i]\n",
    "            alphaDP[t][i]*=b[i][observations[t]]\n",
    "            alphaDpScaleTimeT+=alphaDP[t][i]\n",
    "        #print(\"Scaling \",alphaDpScaleTimeT)\n",
    "        for i in np.arange(statesC):\n",
    "            alphaDP[t][i]/=alphaDpScaleTimeT\n",
    "    #print(\"Next AlphaDP \",alphaDP[1])\n",
    "    #print(\"alphaDP \",alphaDP)\n",
    "def observationsLikelihood(alphaDP):\n",
    "    timePts=alphaDP.shape[0]\n",
    "    stateC=alphaDP.shape[1]\n",
    "    ans=0.0\n",
    "    for i in np.arange(stateC):\n",
    "        ans+=alphaDP[timePts-1][i]\n",
    "        #print(\"Alpha@Timept \",i ,\" : \", alphaDP[i])\n",
    "    #print(\"Observation Likelihood \",ans)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Î²[t][i]: Probability of seeing the observations from t + 1 to end(T), given that we are in state i at time t\n",
    "beta[t][i]= a[i][0]*b[0][o[t+1]]*beta[t+1][0] + a[i][1]*b[1][o[t+1]]*beta[t+1][1] + ......... +\n",
    "                a[i][N-2]*b[N-2][o[t+1]]*beta[t+1][N-2] + a[i][N-1]*b[N-1][o[t+1]]*beta[t+1][N-1]\"\"\"\n",
    "def computeBeta(observations,a,b,pi,betaDP):\n",
    "    statesC=a.shape[0]\n",
    "    timePts=observations.shape[0]\n",
    "    if timePts<1:\n",
    "        return\n",
    "    for state in np.arange(statesC):\n",
    "            betaDP[timePts-1][state]=1\n",
    "    for t in np.arange(timePts-2,-1,-1):\n",
    "        betaDpScaleTimeT=0\n",
    "        for i in np.arange(statesC):\n",
    "            for j in np.arange(statesC):\n",
    "                betaDP[t][i]+=a[i][j]*b[j][observations[t+1]]*betaDP[t+1][j]\n",
    "            betaDpScaleTimeT+=betaDP[t][i]\n",
    "        if t==timePts-2:\n",
    "            print(\"Non Scaled T-2 \",betaDP[t])\n",
    "        print(\"Scale At \",t,\" \",betaDpScaleTimeT)\n",
    "        for i in np.arange(statesC):\n",
    "            betaDP[t][i]/=betaDpScaleTimeT\n",
    "    print(\"BetaDP t-2 \",betaDP[timePts-2])\n",
    "    #print(\"betaDP \",betaDP)\n",
    "    return betaDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def computeDiGammaNum(t,i,j,alphaDP,betaDP,a,b,observations):\n",
    "    return alphaDP[t][i]*a[i][j]*b[j][observations[t+1]]*betaDP[t+1][j]\n",
    "def computeDiGammaDP(alphaDP,betaDP,a,b,observations):\n",
    "    observationsC=alphaDP.shape[0]\n",
    "    statesC=alphaDP.shape[1]\n",
    "    diGammaDP=np.zeros(shape=(statesC,statesC),dtype=float)\n",
    "    diGammaDenom=observationsLikelihood(alphaDP)\n",
    "    #print(\"Observation Likelihood \",diGammaDenom ,\" \",observations)\n",
    "    #print(\"Observation \",observations)\n",
    "    #print(\"DiGammaDenom \",diGammaDenom ,\"Observation : \",observations,\"AlphaDP \",alphaDP)\n",
    "    for i in np.arange(statesC):\n",
    "        for j in np.arange(statesC):\n",
    "                for t in np.arange(observationsC-1):\n",
    "                    #print(\"DiGama \",i,\" \",j,\" \",t,\" \",computeDiGammaNum(t,i,j,alphaDP,betaDP,a,b,observations))\n",
    "                    diGammaDP[i][j]+=computeDiGammaNum(t,i,j,alphaDP,betaDP,a,b,observations)/diGammaDenom\n",
    "    #print(\"diGammaDP \",diGammaDP)\n",
    "    return diGammaDP\n",
    "def computeTransitionProbabilityA(alphaDP,betaDP,a,b,observations):\n",
    "    #print(\"Got A \",a)\n",
    "    statesC=alphaDP.shape[1]\n",
    "    newlyComputedTransitionProbA=np.zeros(shape=(statesC,statesC),dtype=float)\n",
    "    diGammaDP=computeDiGammaDP(alphaDP,betaDP,a,b,observations)\n",
    "    #print(\"DiGamma \",diGammaDP)\n",
    "    diGammaDPSumGrpByJ=np.zeros(shape=(statesC),dtype=float)\n",
    "    #print(\"DiGammaDP \",diGammaDP)\n",
    "    for i in np.arange(statesC):\n",
    "        sumAcrossJ=0.0\n",
    "        for j in np.arange(statesC):\n",
    "            sumAcrossJ+=diGammaDP[i][j]\n",
    "        diGammaDPSumGrpByJ[i]=sumAcrossJ\n",
    "    #print(\"DiGramGrpByJ \",diGammaDPSumGrpByJ)\n",
    "    for i in np.arange(statesC):    \n",
    "        for j in np.arange(statesC):\n",
    "            if (diGammaDPSumGrpByJ[i]==0):\n",
    "                #print(\"Underflow \",i,\" \",diGammaDPSumGrpByJ[i])\n",
    "                newlyComputedTransitionProbA[i][j]=0.0\n",
    "            else:\n",
    "                newlyComputedTransitionProbA[i][j]=diGammaDP[i][j]/diGammaDPSumGrpByJ[i]\n",
    "    #print(\"Updated A \",newlyComputedTransitionProbA)\n",
    "    return newlyComputedTransitionProbA   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def computeGammaNum(t,j,alphaDP,betaDP):\n",
    "    return alphaDP[t][j]*betaDP[t][j]\n",
    "def computeGammaDP(alphaDP,betaDP):\n",
    "    observationsC=alphaDP.shape[0]\n",
    "    statesC=alphaDP.shape[1]\n",
    "    gammaDP=np.zeros(shape=(statesC,observationsC),dtype=float)\n",
    "    gammaDenom=observationsLikelihood(alphaDP)\n",
    "    for i in np.arange(statesC):\n",
    "        for t in np.arange(observationsC):\n",
    "            gammaDP[i][t]=computeGammaNum(t,i,alphaDP,betaDP)/gammaDenom  \n",
    "    return gammaDP\n",
    "def computeObsrProbNum(gammaDP,i,vk,observations):\n",
    "    observationC=len(observations)\n",
    "    obsrProbNum=0.0\n",
    "    for t in np.arange(observationC):\n",
    "        if observations[t]==vk:\n",
    "            obsrProbNum+=gammaDP[i][t]\n",
    "    return obsrProbNum\n",
    "def computeTransitionProbabilityB(alphaDP,betaDP,a,b,observations,observationDict):\n",
    "    statesC=a.shape[0]\n",
    "    observationsC=b.shape[1]\n",
    "    newlyComputedObsrProbB=np.zeros(shape=(statesC,observationsC),dtype=float)\n",
    "    gammaDP=computeGammaDP(alphaDP,betaDP) \n",
    "    #print(\"gammaDP \",gammaDP)\n",
    "    #print(\"ALPHADP : \",alphaDP)\n",
    "    #print(\"BETADP : \",betaDP)\n",
    "    for i in np.arange(statesC):\n",
    "        obsrProbDenom =np.sum(gammaDP[i])\n",
    "        for vk in observationDict:\n",
    "            newlyComputedObsrProbB[i][vk]=computeObsrProbNum(gammaDP,i,vk,observations)/obsrProbDenom\n",
    "    return newlyComputedObsrProbB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Change Convergence Criteria to be more reasonable/Useful\n",
    "def isConverged(count):\n",
    "    if count>=3:\n",
    "        return True\n",
    "    return False\n",
    "def Forward_Backward_EM_Algo(observations,A,B,pi,observationDict):\n",
    "    count=0\n",
    "    updatedA=A\n",
    "    updatedB=B\n",
    "    while isConverged(count)==False:\n",
    "        #Expectation(E)-Step\n",
    "        alphaDP=np.zeros(shape=(observations.shape[0],updatedA.shape[0]))# Count_of_Observations*Count_of_Hidden_States\n",
    "        betaDP=np.zeros(shape=(observations.shape[0],updatedA.shape[0]))# Count_of_Observations*Count_of_Hidden_States\n",
    "        computeAlpha(observations,updatedA,updatedB,pi,alphaDP)\n",
    "        #print(\"AlphaDP Computed ....\")\n",
    "        #print(\"ALPHA-DP \",alphaDP)\n",
    "        computeBeta(observations,updatedA,updatedB,pi,betaDP)\n",
    "        #Maximization(M)-Step\n",
    "        newA=computeTransitionProbabilityA(alphaDP,betaDP,updatedA,updatedB,observations)\n",
    "        newB=computeTransitionProbabilityB(alphaDP,betaDP,updatedA,updatedB,observations,observationDict)\n",
    "        #print(\"New A =================================>\")\n",
    "        #print(newA)\n",
    "        #print(\"New B =================================>\")\n",
    "        #print(newB)\n",
    "        updatedA=newA\n",
    "        updatedB=newB\n",
    "        count=count+1\n",
    "    return (updatedA,updatedB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trainHMM(trainDataFile,A,B,pi,maxSequences=-1):\n",
    "    trainFile=open(trainDataFile,\"r\")\n",
    "    metaDataLine = trainFile.readline()\n",
    "    headerLine = metaDataLine.split(\" \")\n",
    "    numSequences = int(headerLine[0])\n",
    "    distinctObservations= int(headerLine[1])#Total Number of Distinct Observations\n",
    "    observationDict=np.arange(distinctObservations)\n",
    "    updatedA=np.NaN\n",
    "    updatedB=np.NaN\n",
    "    isAUpdated=False\n",
    "    #for n in range(numSequences):\n",
    "    if(maxSequences==-1):\n",
    "        usedSeqs=numSequences\n",
    "    else:\n",
    "        usedSeqs=min(maxSequences,numSequences)\n",
    "    actuallyUsedSeqs=0\n",
    "    for n in range(usedSeqs):\n",
    "        line = trainFile.readline()#Reading Sequences 1 by 1\n",
    "        l = line.split(\" \")\n",
    "        #print(\"For Sequence \",n,\" =====================================>\")\n",
    "        if(int(l[0])<=1):\n",
    "            #print(\"Skipping \",l)\n",
    "            continue\n",
    "        actuallyUsedSeqs+=1\n",
    "        observations=np.array([int(i) for i in l[1:len(l)]])\n",
    "        learnedParams=Forward_Backward_EM_Algo(observations,A,B,pi,observationDict)\n",
    "        if isAUpdated==False:\n",
    "            isAUpdated=True\n",
    "            updatedA=learnedParams[0]\n",
    "            updatedB=learnedParams[1]\n",
    "        else:\n",
    "            updatedA+=learnedParams[0]\n",
    "            updatedB+=learnedParams[1]\n",
    "        #print(\"State Transition Matrix (A) ===>\")\n",
    "        #print(learnedParams[0])\n",
    "        #print(\"Observation Probability Matrix (B) ===>\")\n",
    "        #print(learnedParams[1])\n",
    "        #print(\"Aggregated State Transition Matrix (A) ===>\")\n",
    "        #print(updatedA)\n",
    "        #print(\"Aggregated Observation Probability Matrix (B) ===>\")\n",
    "        #print(updatedB)\n",
    "    updatedA=updatedA/actuallyUsedSeqs\n",
    "    updatedB=updatedB/actuallyUsedSeqs\n",
    "    return (updatedA,updatedB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def trainModel(fileLoc,maxNoOfStates,maxSequences=-1):\n",
    "    start = time.time()\n",
    "    initialProbs=computeInitialProb(fileLoc,maxNoOfStates)\n",
    "    end = time.time()\n",
    "    print(\"Computed Initial Prob. in \", end - start ,\"seconds\")\n",
    "    pi=initialProbs[2]\n",
    "    numOfStates=initialProbs[0]\n",
    "    distinctObservations=initialProbs[1]\n",
    "    #print(initialProbs)\n",
    "    A=createRandomMatrixA(numOfStates)\n",
    "    B=createRandomMatrixB(numOfStates,distinctObservations)\n",
    "    #print(A)\n",
    "    #print(B)\n",
    "    trainedParams=trainHMM(fileLoc,A,B,pi,maxSequences)\n",
    "    end=time.time()\n",
    "    #print(\"For \",maxSequences,\" Sequences : Total Training Time \",end-start)\n",
    "    return trainedParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed Initial Prob. in  0.34983348846435547 seconds\n",
      "Non Scaled T-2  [ 0.000125  0.000125  0.000125  0.000125  0.000125  0.000125  0.000125\n",
      "  0.000125  0.000125  0.000125  0.000125  0.000125  0.000125  0.000125\n",
      "  0.000125  0.000125  0.000125  0.000125  0.000125  0.000125]\n",
      "Scale At  72   0.0025\n",
      "Scale At  71   0.000125\n",
      "Scale At  70   0.000125\n",
      "Scale At  69   0.000125\n",
      "Scale At  68   0.000125\n",
      "Scale At  67   0.000125\n",
      "Scale At  66   0.000125\n",
      "Scale At  65   0.000125\n",
      "Scale At  64   0.000125\n",
      "Scale At  63   0.000125\n",
      "Scale At  62   0.000125\n",
      "Scale At  61   0.000125\n",
      "Scale At  60   0.000125\n",
      "Scale At  59   0.000125\n",
      "Scale At  58   0.000125\n",
      "Scale At  57   0.000125\n",
      "Scale At  56   0.000125\n",
      "Scale At  55   0.000125\n",
      "Scale At  54   0.000125\n",
      "Scale At  53   0.000125\n",
      "Scale At  52   0.000125\n",
      "Scale At  51   0.000125\n",
      "Scale At  50   0.000125\n",
      "Scale At  49   0.000125\n",
      "Scale At  48   0.000125\n",
      "Scale At  47   0.000125\n",
      "Scale At  46   0.000125\n",
      "Scale At  45   0.000125\n",
      "Scale At  44   0.000125\n",
      "Scale At  43   0.000125\n",
      "Scale At  42   0.000125\n",
      "Scale At  41   0.000125\n",
      "Scale At  40   0.000125\n",
      "Scale At  39   0.000125\n",
      "Scale At  38   0.000125\n",
      "Scale At  37   0.000125\n",
      "Scale At  36   0.000125\n",
      "Scale At  35   0.000125\n",
      "Scale At  34   0.000125\n",
      "Scale At  33   0.000125\n",
      "Scale At  32   0.000125\n",
      "Scale At  31   0.000125\n",
      "Scale At  30   0.000125\n",
      "Scale At  29   0.000125\n",
      "Scale At  28   0.000125\n",
      "Scale At  27   0.000125\n",
      "Scale At  26   0.000125\n",
      "Scale At  25   0.000125\n",
      "Scale At  24   0.000125\n",
      "Scale At  23   0.000125\n",
      "Scale At  22   0.000125\n",
      "Scale At  21   0.000125\n",
      "Scale At  20   0.000125\n",
      "Scale At  19   0.000125\n",
      "Scale At  18   0.000125\n",
      "Scale At  17   0.000125\n",
      "Scale At  16   0.000125\n",
      "Scale At  15   0.000125\n",
      "Scale At  14   0.000125\n",
      "Scale At  13   0.000125\n",
      "Scale At  12   0.000125\n",
      "Scale At  11   0.000125\n",
      "Scale At  10   0.000125\n",
      "Scale At  9   0.000125\n",
      "Scale At  8   0.000125\n",
      "Scale At  7   0.000125\n",
      "Scale At  6   0.000125\n",
      "Scale At  5   0.000125\n",
      "Scale At  4   0.000125\n",
      "Scale At  3   0.000125\n",
      "Scale At  2   0.000125\n",
      "Scale At  1   0.000125\n",
      "Scale At  0   0.000125\n",
      "BetaDP t-2  [ 0.05  0.05  0.05  0.05  0.05  0.05  0.05  0.05  0.05  0.05  0.05  0.05\n",
      "  0.05  0.05  0.05  0.05  0.05  0.05  0.05  0.05]\n",
      "Non Scaled T-2  [ 0.22581767  0.22581767  0.22581767  0.22581767  0.22581767  0.22581767\n",
      "  0.22581767  0.22581767  0.22581767  0.22581767  0.22581767  0.22581767\n",
      "  0.22581767  0.22581767  0.22581767  0.22581767  0.22581767  0.22581767\n",
      "  0.22581767  0.22581767]\n",
      "Scale At  72   4.51635337552\n",
      "Scale At  71   0.129038667872\n",
      "Scale At  70   0.0107532223227\n",
      "Scale At  69   0.0644696579273\n",
      "Scale At  68   0.064519333936\n",
      "Scale At  67   0.129038667872\n",
      "Scale At  66   0.129038667872\n",
      "Scale At  65   0.129038667872\n",
      "Scale At  64   0.0430128892907\n",
      "Scale At  63   0.064519333936\n",
      "Scale At  62   0.129038667872\n",
      "Scale At  61   0.032259666968\n",
      "Scale At  60   0.129038667872\n",
      "Scale At  59   0.064519333936\n",
      "Scale At  58   0.129038667872\n",
      "Scale At  57   0.032259666968\n",
      "Scale At  56   0.107532223227\n",
      "Scale At  55   0.032259666968\n",
      "Scale At  54   0.0644696579273\n",
      "Scale At  53   0.107532223227\n",
      "Scale At  52   0.032259666968\n",
      "Scale At  51   0.032259666968\n",
      "Scale At  50   0.107532223227\n",
      "Scale At  49   0.0430128892907\n",
      "Scale At  48   0.032259666968\n",
      "Scale At  47   0.0107532223227\n",
      "Scale At  46   0.129038667872\n",
      "Scale At  45   0.107532223227\n",
      "Scale At  44   0.129038667872\n",
      "Scale At  43   0.0430128892907\n",
      "Scale At  42   0.0644696579273\n",
      "Scale At  41   0.032259666968\n",
      "Scale At  40   0.0430128892907\n",
      "Scale At  39   0.0430128892907\n",
      "Scale At  38   0.0430128892907\n",
      "Scale At  37   0.0107532223227\n",
      "Scale At  36   0.0430128892907\n",
      "Scale At  35   0.0430128892907\n",
      "Scale At  34   0.0644696579273\n",
      "Scale At  33   0.0215064446453\n",
      "Scale At  32   0.107532223227\n",
      "Scale At  31   0.107532223227\n",
      "Scale At  30   0.0430128892907\n",
      "Scale At  29   0.129038667872\n",
      "Scale At  28   0.032259666968\n",
      "Scale At  27   0.0107532223227\n",
      "Scale At  26   0.064519333936\n",
      "Scale At  25   0.0430128892907\n",
      "Scale At  24   0.0644696579273\n",
      "Scale At  23   0.032259666968\n",
      "Scale At  22   0.0430128892907\n",
      "Scale At  21   0.0430128892907\n",
      "Scale At  20   0.064519333936\n",
      "Scale At  19   0.0430128892907\n",
      "Scale At  18   0.107532223227\n",
      "Scale At  17   0.107532223227\n",
      "Scale At  16   0.0430128892907\n",
      "Scale At  15   0.0430128892907\n",
      "Scale At  14   0.107532223227\n",
      "Scale At  13   0.225817668776\n",
      "Scale At  12   0.0430128892907\n",
      "Scale At  11   0.0215064446453\n",
      "Scale At  10   0.129038667872\n",
      "Scale At  9   0.0430128892907\n",
      "Scale At  8   0.032259666968\n",
      "Scale At  7   0.064519333936\n",
      "Scale At  6   0.129038667872\n",
      "Scale At  5   0.0430128892907\n",
      "Scale At  4   0.0430128892907\n",
      "Scale At  3   0.107532223227\n",
      "Scale At  2   0.032259666968\n",
      "Scale At  1   0.032259666968\n",
      "Scale At  0   0.0430128892907\n",
      "BetaDP t-2  [ 0.05  0.05  0.05  0.05  0.05  0.05  0.05  0.05  0.05  0.05  0.05  0.05\n",
      "  0.05  0.05  0.05  0.05  0.05  0.05  0.05  0.05]\n",
      "Non Scaled T-2  [ 0.22584179  0.22584175  0.22584177  0.22584189  0.22584181  0.22584182\n",
      "  0.22584195  0.2258418   0.22584176  0.22584181  0.22584176  0.2258418\n",
      "  0.22584182  0.22584185  0.22584186  0.22584204  0.22584178  0.22584177\n",
      "  0.22584184  0.22584181]\n",
      "Scale At  72   4.51683647148\n",
      "Scale At  71   0.129052469918\n",
      "Scale At  70   0.0107543724931\n",
      "Scale At  69   0.0643695920924\n",
      "Scale At  68   0.0645262403642\n",
      "Scale At  67   0.129052469918\n",
      "Scale At  66   0.129052469918\n",
      "Scale At  65   0.129052469918\n",
      "Scale At  64   0.0430174899726\n",
      "Scale At  63   0.0645262349589\n",
      "Scale At  62   0.129052469918\n",
      "Scale At  61   0.0322631174794\n",
      "Scale At  60   0.129052469918\n",
      "Scale At  59   0.0645262349589\n",
      "Scale At  58   0.129052469918\n",
      "Scale At  57   0.0322631174794\n",
      "Scale At  56   0.107543724931\n",
      "Scale At  55   0.0322631174794\n",
      "Scale At  54   0.0643695920924\n",
      "Scale At  53   0.10754373394\n",
      "Scale At  52   0.0322631174794\n",
      "Scale At  51   0.0322631174794\n",
      "Scale At  50   0.107543724931\n",
      "Scale At  49   0.0430174899726\n",
      "Scale At  48   0.0322631174794\n",
      "Scale At  47   0.0107543724931\n",
      "Scale At  46   0.129052469918\n",
      "Scale At  45   0.107543724931\n",
      "Scale At  44   0.129052469918\n",
      "Scale At  43   0.0430174899726\n",
      "Scale At  42   0.0643695920924\n",
      "Scale At  41   0.0322631201821\n",
      "Scale At  40   0.0430174899725\n",
      "Scale At  39   0.0430174899726\n",
      "Scale At  38   0.0430174899726\n",
      "Scale At  37   0.0107543724931\n",
      "Scale At  36   0.0430174899726\n",
      "Scale At  35   0.0430174899726\n",
      "Scale At  34   0.0643695920924\n",
      "Scale At  33   0.0215087467881\n",
      "Scale At  32   0.107543724931\n",
      "Scale At  31   0.107543724931\n",
      "Scale At  30   0.0430174899726\n",
      "Scale At  29   0.129052469918\n",
      "Scale At  28   0.0322631174794\n",
      "Scale At  27   0.0107543724931\n",
      "Scale At  26   0.0645262349589\n",
      "Scale At  25   0.0430174899726\n",
      "Scale At  24   0.0643695920924\n",
      "Scale At  23   0.0322631201821\n",
      "Scale At  22   0.0430174899725\n",
      "Scale At  21   0.0430174899726\n",
      "Scale At  20   0.0645262349589\n",
      "Scale At  19   0.0430174899726\n",
      "Scale At  18   0.107543724931\n",
      "Scale At  17   0.107543724931\n",
      "Scale At  16   0.0430174899726\n",
      "Scale At  15   0.0430174899726\n",
      "Scale At  14   0.107543724931\n",
      "Scale At  13   0.225841822356\n",
      "Scale At  12   0.0430174899726\n",
      "Scale At  11   0.0215087449863\n",
      "Scale At  10   0.129052469918\n",
      "Scale At  9   0.0430174899726\n",
      "Scale At  8   0.0322631174794\n",
      "Scale At  7   0.0645262349589\n",
      "Scale At  6   0.129052469918\n",
      "Scale At  5   0.0430174899726\n",
      "Scale At  4   0.0430174899726\n",
      "Scale At  3   0.107543724931\n",
      "Scale At  2   0.0322631174794\n",
      "Scale At  1   0.0322631174794\n",
      "Scale At  0   0.0430174899726\n",
      "BetaDP t-2  [ 0.04999999  0.04999998  0.04999999  0.05000001  0.05        0.05\n",
      "  0.05000003  0.04999999  0.04999999  0.05        0.04999999  0.05        0.05\n",
      "  0.05000001  0.05000001  0.05000005  0.04999999  0.04999999  0.05        0.05      ]\n"
     ]
    }
   ],
   "source": [
    "(A,B)=trainModel('Data/1.spice.train.txt',20,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0511838 ,  0.05212298,  0.05177876,  0.04850927,  0.05094284,\n",
       "         0.05065418,  0.04526996,  0.0511085 ,  0.05185294,  0.05077235,\n",
       "         0.05190006,  0.05095403,  0.05058719,  0.04970177,  0.04949974,\n",
       "         0.0390022 ,  0.05149016,  0.05173075,  0.05008667,  0.05085185],\n",
       "       [ 0.05118405,  0.0521236 ,  0.05177923,  0.04850877,  0.05094301,\n",
       "         0.05065425,  0.04526885,  0.05110872,  0.05185345,  0.05077246,\n",
       "         0.05190059,  0.0509542 ,  0.05058724,  0.04970156,  0.04949947,\n",
       "         0.03900027,  0.05149052,  0.0517312 ,  0.05008657,  0.05085198],\n",
       "       [ 0.05118396,  0.05212336,  0.05177905,  0.04850896,  0.05094294,\n",
       "         0.05065422,  0.04526928,  0.05110864,  0.05185326,  0.05077242,\n",
       "         0.05190038,  0.05095413,  0.05058722,  0.04970164,  0.04949958,\n",
       "         0.03900101,  0.05149038,  0.05173103,  0.05008661,  0.05085193],\n",
       "       [ 0.05118324,  0.05212159,  0.0517777 ,  0.04851039,  0.05094246,\n",
       "         0.05065402,  0.04527247,  0.051108  ,  0.05185181,  0.0507721 ,\n",
       "         0.05189889,  0.05095364,  0.05058707,  0.04970223,  0.04950032,\n",
       "         0.03900655,  0.05148935,  0.05172973,  0.05008689,  0.05085154],\n",
       "       [ 0.05118375,  0.05212283,  0.05177865,  0.04850939,  0.0509428 ,\n",
       "         0.05065416,  0.04527023,  0.05110845,  0.05185283,  0.05077232,\n",
       "         0.05189994,  0.05095399,  0.05058718,  0.04970182,  0.0494998 ,\n",
       "         0.03900266,  0.05149007,  0.05173064,  0.0500867 ,  0.05085181],\n",
       "       [ 0.05118368,  0.05212266,  0.05177852,  0.04850952,  0.05094276,\n",
       "         0.05065414,  0.04527053,  0.05110839,  0.05185269,  0.05077229,\n",
       "         0.0518998 ,  0.05095394,  0.05058716,  0.04970187,  0.04949987,\n",
       "         0.03900318,  0.05148998,  0.05173052,  0.05008672,  0.05085178],\n",
       "       [ 0.05118273,  0.05212032,  0.05177672,  0.04851142,  0.05094212,\n",
       "         0.05065386,  0.04527477,  0.05110754,  0.05185078,  0.05077187,\n",
       "         0.05189781,  0.05095329,  0.05058696,  0.04970265,  0.04950085,\n",
       "         0.03901056,  0.0514886 ,  0.05172879,  0.05008709,  0.05085125],\n",
       "       [ 0.05118379,  0.05212293,  0.05177872,  0.04850931,  0.05094283,\n",
       "         0.05065417,  0.04527005,  0.05110848,  0.05185291,  0.05077234,\n",
       "         0.05190002,  0.05095401,  0.05058719,  0.04970179,  0.04949976,\n",
       "         0.03900235,  0.05149013,  0.05173071,  0.05008668,  0.05085184],\n",
       "       [ 0.05118398,  0.05212341,  0.05177909,  0.04850892,  0.05094296,\n",
       "         0.05065423,  0.04526919,  0.05110866,  0.0518533 ,  0.05077243,\n",
       "         0.05190043,  0.05095414,  0.05058722,  0.04970163,  0.04949955,\n",
       "         0.03900086,  0.05149041,  0.05173107,  0.0500866 ,  0.05085194],\n",
       "       [ 0.05118371,  0.05212273,  0.05177857,  0.04850947,  0.05094277,\n",
       "         0.05065415,  0.04527041,  0.05110841,  0.05185274,  0.05077231,\n",
       "         0.05189985,  0.05095396,  0.05058717,  0.04970185,  0.04949984,\n",
       "         0.03900297,  0.05149002,  0.05173057,  0.05008671,  0.05085179],\n",
       "       [ 0.05118399,  0.05212344,  0.05177911,  0.04850889,  0.05094296,\n",
       "         0.05065423,  0.04526913,  0.05110867,  0.05185332,  0.05077243,\n",
       "         0.05190045,  0.05095415,  0.05058723,  0.04970161,  0.04949954,\n",
       "         0.03900076,  0.05149043,  0.05173109,  0.0500866 ,  0.05085195],\n",
       "       [ 0.05118375,  0.05212284,  0.05177865,  0.04850938,  0.0509428 ,\n",
       "         0.05065416,  0.04527021,  0.05110845,  0.05185283,  0.05077233,\n",
       "         0.05189994,  0.05095399,  0.05058718,  0.04970182,  0.04949979,\n",
       "         0.03900264,  0.05149008,  0.05173065,  0.0500867 ,  0.05085182],\n",
       "       [ 0.05118366,  0.05212263,  0.05177849,  0.04850955,  0.05094275,\n",
       "         0.05065414,  0.0452706 ,  0.05110837,  0.05185266,  0.05077229,\n",
       "         0.05189976,  0.05095393,  0.05058716,  0.04970189,  0.04949988,\n",
       "         0.0390033 ,  0.05148995,  0.05173049,  0.05008673,  0.05085177],\n",
       "       [ 0.05118347,  0.05212215,  0.05177813,  0.04850994,  0.05094262,\n",
       "         0.05065408,  0.04527145,  0.0511082 ,  0.05185227,  0.0507722 ,\n",
       "         0.05189937,  0.0509538 ,  0.05058712,  0.04970204,  0.04950008,\n",
       "         0.03900478,  0.05148968,  0.05173014,  0.05008681,  0.05085166],\n",
       "       [ 0.05118343,  0.05212205,  0.05177805,  0.04851002,  0.05094259,\n",
       "         0.05065407,  0.04527163,  0.05110817,  0.05185219,  0.05077218,\n",
       "         0.05189928,  0.05095377,  0.05058711,  0.04970208,  0.04950012,\n",
       "         0.03900509,  0.05148962,  0.05173007,  0.05008682,  0.05085164],\n",
       "       [ 0.05118194,  0.0521184 ,  0.05177525,  0.04851298,  0.05094159,\n",
       "         0.05065363,  0.04527826,  0.05110683,  0.05184921,  0.05077152,\n",
       "         0.05189618,  0.05095275,  0.0505868 ,  0.04970329,  0.04950165,\n",
       "         0.03901665,  0.05148747,  0.05172738,  0.05008739,  0.05085082],\n",
       "       [ 0.05118388,  0.05212317,  0.05177891,  0.04850911,  0.05094289,\n",
       "         0.0506542 ,  0.04526962,  0.05110857,  0.0518531 ,  0.05077238,\n",
       "         0.05190022,  0.05095408,  0.05058721,  0.04970171,  0.04949966,\n",
       "         0.0390016 ,  0.05149027,  0.05173089,  0.05008664,  0.05085189],\n",
       "       [ 0.05118395,  0.05212333,  0.05177903,  0.04850899,  0.05094293,\n",
       "         0.05065422,  0.04526933,  0.05110863,  0.05185323,  0.05077241,\n",
       "         0.05190036,  0.05095412,  0.05058722,  0.04970165,  0.04949959,\n",
       "         0.03900111,  0.05149036,  0.05173101,  0.05008662,  0.05085192],\n",
       "       [ 0.05118355,  0.05212235,  0.05177828,  0.04850978,  0.05094267,\n",
       "         0.05065411,  0.04527109,  0.05110827,  0.05185244,  0.05077224,\n",
       "         0.05189953,  0.05095385,  0.05058714,  0.04970198,  0.0495    ,\n",
       "         0.03900415,  0.05148979,  0.05173029,  0.05008677,  0.05085171],\n",
       "       [ 0.05118372,  0.05212278,  0.05177861,  0.04850943,  0.05094279,\n",
       "         0.05065416,  0.04527032,  0.05110843,  0.05185278,  0.05077231,\n",
       "         0.05189989,  0.05095397,  0.05058717,  0.04970184,  0.04949982,\n",
       "         0.03900282,  0.05149004,  0.0517306 ,  0.05008671,  0.0508518 ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed Initial Prob. in  0.4529531002044678 seconds\n",
      "(20, 20, [0.036700000000000003, 0.0091999999999999998, 0.020549999999999999, 0.081900000000000001, 0.042299999999999997, 0.04845, 0.11260000000000001, 0.0385, 0.018249999999999999, 0.045999999999999999, 0.016750000000000001, 0.042049999999999997, 0.049799999999999997, 0.065449999999999994, 0.06855, 0.1487, 0.028850000000000001, 0.021999999999999999, 0.0591, 0.044299999999999999])\n",
      "For  15  Sequences : Total Training Time  51.64145803451538\n",
      "Computed Initial Prob. in  0.2604796886444092 seconds\n",
      "(20, 20, [0.036700000000000003, 0.0091999999999999998, 0.020549999999999999, 0.081900000000000001, 0.042299999999999997, 0.04845, 0.11260000000000001, 0.0385, 0.018249999999999999, 0.045999999999999999, 0.016750000000000001, 0.042049999999999997, 0.049799999999999997, 0.065449999999999994, 0.06855, 0.1487, 0.028850000000000001, 0.021999999999999999, 0.0591, 0.044299999999999999])\n",
      "For  15  Sequences : Total Training Time  45.84242844581604\n",
      "Computed Initial Prob. in  0.207383394241333 seconds\n",
      "(20, 20, [0.036700000000000003, 0.0091999999999999998, 0.020549999999999999, 0.081900000000000001, 0.042299999999999997, 0.04845, 0.11260000000000001, 0.0385, 0.018249999999999999, 0.045999999999999999, 0.016750000000000001, 0.042049999999999997, 0.049799999999999997, 0.065449999999999994, 0.06855, 0.1487, 0.028850000000000001, 0.021999999999999999, 0.0591, 0.044299999999999999])\n",
      "For  15  Sequences : Total Training Time  44.81167411804199\n",
      "Computed Initial Prob. in  0.22207403182983398 seconds\n",
      "(20, 20, [0.036700000000000003, 0.0091999999999999998, 0.020549999999999999, 0.081900000000000001, 0.042299999999999997, 0.04845, 0.11260000000000001, 0.0385, 0.018249999999999999, 0.045999999999999999, 0.016750000000000001, 0.042049999999999997, 0.049799999999999997, 0.065449999999999994, 0.06855, 0.1487, 0.028850000000000001, 0.021999999999999999, 0.0591, 0.044299999999999999])\n",
      "For  15  Sequences : Total Training Time  45.1577582359314\n",
      "1 loops, best of 3: 44.8 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit trainModel('Data/1.spice.train.txt',20,15)\n",
    "#(A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed Initial Prob. in  0.21947813034057617 seconds\n",
      "(20, 20, [0.036700000000000003, 0.0091999999999999998, 0.020549999999999999, 0.081900000000000001, 0.042299999999999997, 0.04845, 0.11260000000000001, 0.0385, 0.018249999999999999, 0.045999999999999999, 0.016750000000000001, 0.042049999999999997, 0.049799999999999997, 0.065449999999999994, 0.06855, 0.1487, 0.028850000000000001, 0.021999999999999999, 0.0591, 0.044299999999999999])\n",
      "For  150  Sequences : Total Training Time  513.4503910541534\n"
     ]
    }
   ],
   "source": [
    "(A,B)=trainModel('Data/1.spice.train.txt',20,150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed Initial Prob. in  0.17683792114257812 seconds\n",
      "(20, 20, [0.036700000000000003, 0.0091999999999999998, 0.020549999999999999, 0.081900000000000001, 0.042299999999999997, 0.04845, 0.11260000000000001, 0.0385, 0.018249999999999999, 0.045999999999999999, 0.016750000000000001, 0.042049999999999997, 0.049799999999999997, 0.065449999999999994, 0.06855, 0.1487, 0.028850000000000001, 0.021999999999999999, 0.0591, 0.044299999999999999])\n",
      "For  1500  Sequences : Total Training Time  6131.2775712013245\n"
     ]
    }
   ],
   "source": [
    "(A,B)=trainModel('Data/1.spice.train.txt',20,1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#(A,B)=trainModel('Data/1.spice.train.txt',20,1600)\n",
    "#(A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
